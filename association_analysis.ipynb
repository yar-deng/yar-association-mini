{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6fec4aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulated Transactions:\n",
      "  Transaction                                   Items\n",
      "0          T1          [Bananas, Milk, Yogurt, Bread]\n",
      "1          T2           [Bread, Milk, Yogurt, Butter]\n",
      "2          T3                          [Cheese, Eggs]\n",
      "3          T4  [Bananas, Eggs, Apples, Bread, Butter]\n",
      "4          T5          [Bread, Bananas, Eggs, Cheese]\n",
      "5          T6                   [Milk, Cheese, Bread]\n",
      "6          T7                        [Yogurt, Butter]\n",
      "7          T8                         [Eggs, Bananas]\n",
      "8          T9                    [Milk, Cheese, Eggs]\n",
      "9         T10                 [Bananas, Eggs, Cheese]\n",
      "\n",
      "One-Hot Encoded DataFrame:\n",
      "   Bread  Milk  Eggs  Butter  Cheese  Yogurt  Apples  Bananas\n",
      "0      1     1     0       0       0       1       0        1\n",
      "1      1     1     0       1       0       1       0        0\n",
      "2      0     0     1       0       1       0       0        0\n",
      "3      1     0     1       1       0       0       1        1\n",
      "4      1     0     1       0       1       0       0        1\n",
      "5      1     1     0       0       1       0       0        0\n",
      "6      0     0     0       1       0       1       0        0\n",
      "7      0     0     1       0       0       0       0        1\n",
      "8      0     1     1       0       1       0       0        0\n",
      "9      0     0     1       0       1       0       0        1\n",
      "\n",
      "Frequent Itemsets (min_support=0.3):\n",
      "            itemsets  support\n",
      "0            {Bread}      0.5\n",
      "1             {Milk}      0.4\n",
      "2             {Eggs}      0.6\n",
      "3           {Butter}      0.3\n",
      "4           {Cheese}      0.5\n",
      "5           {Yogurt}      0.3\n",
      "6          {Bananas}      0.5\n",
      "7      {Bread, Milk}      0.3\n",
      "8   {Bananas, Bread}      0.3\n",
      "9     {Cheese, Eggs}      0.4\n",
      "10   {Bananas, Eggs}      0.4\n",
      "\n",
      "Association Rules (min_confidence=0.7):\n",
      "  antecedents consequents  support  confidence\n",
      "0      {Milk}     {Bread}      0.3        0.75\n",
      "1    {Cheese}      {Eggs}      0.4        0.80\n",
      "\n",
      "Explanation of Rule: Milk -> Bread\n",
      "This rule means that when Milk is purchased, there is a 75.0% chance that Bread will also be purchased. In everyday terms, if a customer buys Milk, they are highly likely to buy Bread as well, suggesting a strong shopping pattern, possibly because these items are often used together (e.g., in a recipe).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "\n",
    "# 1. Simulate Transaction Data\n",
    "np.random.seed(42)  # For reproducibility\n",
    "items = ['Bread', 'Milk', 'Eggs', 'Butter', 'Cheese', 'Yogurt', 'Apples', 'Bananas']\n",
    "transactions = []\n",
    "for _ in range(10):  # Generate 10 transactions\n",
    "    num_items = np.random.randint(2, 6)  # 2-5 items per transaction\n",
    "    transaction = np.random.choice(items, size=num_items, replace=False).tolist()\n",
    "    transactions.append(transaction)\n",
    "\n",
    "# Create a DataFrame to display transactions\n",
    "transaction_df = pd.DataFrame({'Transaction': [f'T{i+1}' for i in range(len(transactions))], 'Items': transactions})\n",
    "print(\"Simulated Transactions:\")\n",
    "print(transaction_df)\n",
    "\n",
    "# 2. One-Hot Encoding\n",
    "# Convert transactions to one-hot encoded format\n",
    "def encode_transactions(transactions, items):\n",
    "    encoded_vals = []\n",
    "    for transaction in transactions:\n",
    "        labels = {item: 0 for item in items}\n",
    "        for item in transaction:\n",
    "            labels[item] = 1\n",
    "        encoded_vals.append(labels)\n",
    "    return pd.DataFrame(encoded_vals)\n",
    "\n",
    "one_hot_df = encode_transactions(transactions, items)\n",
    "print(\"\\nOne-Hot Encoded DataFrame:\")\n",
    "print(one_hot_df)\n",
    "\n",
    "# 3. Custom Apriori Algorithm\n",
    "def get_frequent_itemsets(df, min_support=0.3):\n",
    "    n_transactions = len(df)\n",
    "    min_support_count = min_support * n_transactions\n",
    "    \n",
    "    # Start with single items\n",
    "    itemsets = {frozenset([item]): df[item].sum() / n_transactions for item in df.columns}\n",
    "    frequent_itemsets = {k: v for k, v in itemsets.items() if v >= min_support}\n",
    "    \n",
    "    # Generate itemsets of increasing size\n",
    "    k = 2\n",
    "    while True:\n",
    "        new_itemsets = {}\n",
    "        # Generate combinations of frequent itemsets of size k-1\n",
    "        prev_items = list(frequent_itemsets.keys())\n",
    "        for i in range(len(prev_items)):\n",
    "            for j in range(i + 1, len(prev_items)):\n",
    "                # Combine itemsets if they share k-2 items\n",
    "                items1, items2 = prev_items[i], prev_items[j]\n",
    "                union = items1 | items2\n",
    "                if len(union) == k:\n",
    "                    # Calculate support\n",
    "                    support = np.sum(df[list(union)].min(axis=1)) / n_transactions\n",
    "                    if support >= min_support:\n",
    "                        new_itemsets[union] = support\n",
    "        if not new_itemsets:\n",
    "            break\n",
    "        frequent_itemsets.update(new_itemsets)\n",
    "        k += 1\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    frequent_itemsets_df = pd.DataFrame({\n",
    "        'itemsets': [set(itemset) for itemset in frequent_itemsets.keys()],\n",
    "        'support': list(frequent_itemsets.values())\n",
    "    })\n",
    "    return frequent_itemsets_df\n",
    "\n",
    "frequent_itemsets = get_frequent_itemsets(one_hot_df, min_support=0.3)\n",
    "print(\"\\nFrequent Itemsets (min_support=0.3):\")\n",
    "print(frequent_itemsets)\n",
    "\n",
    "# 4. Generate Association Rules\n",
    "def generate_association_rules(frequent_itemsets, df, min_confidence=0.7):\n",
    "    rules = []\n",
    "    n_transactions = len(df)\n",
    "    \n",
    "    for _, row in frequent_itemsets.iterrows():\n",
    "        itemset = row['itemsets']\n",
    "        support_itemset = row['support']\n",
    "        \n",
    "        # Generate all possible non-empty subsets as antecedents\n",
    "        for i in range(1, len(itemset)):\n",
    "            for antecedent in combinations(itemset, i):\n",
    "                antecedent = frozenset(antecedent)\n",
    "                consequent = frozenset(itemset - antecedent)\n",
    "                if not consequent:\n",
    "                    continue\n",
    "                \n",
    "                # Calculate support of antecedent\n",
    "                support_antecedent = np.sum(df[list(antecedent)].min(axis=1)) / n_transactions\n",
    "                if support_antecedent == 0:\n",
    "                    continue\n",
    "                \n",
    "                # Calculate confidence\n",
    "                confidence = support_itemset / support_antecedent\n",
    "                if confidence >= min_confidence:\n",
    "                    rules.append({\n",
    "                        'antecedents': set(antecedent),\n",
    "                        'consequents': set(consequent),\n",
    "                        'support': support_itemset,\n",
    "                        'confidence': confidence\n",
    "                    })\n",
    "    \n",
    "    rules_df = pd.DataFrame(rules)\n",
    "    if rules_df.empty:\n",
    "        return rules_df\n",
    "    return rules_df[['antecedents', 'consequents', 'support', 'confidence']]\n",
    "\n",
    "rules = generate_association_rules(frequent_itemsets, one_hot_df, min_confidence=0.7)\n",
    "print(\"\\nAssociation Rules (min_confidence=0.7):\")\n",
    "print(rules.head(2))\n",
    "\n",
    "# 5. Explanation of one rule\n",
    "if not rules.empty:\n",
    "    first_rule = rules.iloc[0]\n",
    "    antecedents = ', '.join(list(first_rule['antecedents']))\n",
    "    consequents = ', '.join(list(first_rule['consequents']))\n",
    "    confidence = first_rule['confidence']\n",
    "    print(f\"\\nExplanation of Rule: {antecedents} -> {consequents}\")\n",
    "    print(f\"This rule means that when {antecedents} is purchased, there is a {confidence*100:.1f}% chance that {consequents} will also be purchased. \"\n",
    "          f\"In everyday terms, if a customer buys {antecedents}, they are highly likely to buy {consequents} as well, \"\n",
    "          f\"suggesting a strong shopping pattern, possibly because these items are often used together (e.g., in a recipe).\")\n",
    "else:\n",
    "    print(\"\\nNo association rules found with the specified confidence threshold.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
